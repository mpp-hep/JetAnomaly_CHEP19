{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py, sys, glob\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('sonic.mplstyle')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "fileIN = shuffle(glob.glob(\"../data/training/qcd*SIDEBAND*.h5\"), random_state=1111)\n",
    "i_train = int(0.5*len(fileIN))\n",
    "i_test = int(0.75*len(fileIN))\n",
    "X_train = fileIN[:i_train]\n",
    "X_test = fileIN[i_train:i_test]\n",
    "print(len(fileIN), len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minN = 99999999\n",
    "#for fname in fileIN:\n",
    "#    f = h5py.File(fname)\n",
    "#    d = f.get(\"EFP\")\n",
    "#    minN = min(minN,d.shape[0])\n",
    "#print(minN)\n",
    "minN = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputShape = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputArray = Input(shape=(inputShape,))\n",
    "x = BatchNormalization()(inputArray)\n",
    "\n",
    "x = Dense(50, activation=\"relu\", kernel_initializer='lecun_uniform', name='enc_0')(x)\n",
    "#x = Dropout(self.dropout)(x)\n",
    "x = Dense(25, activation=\"relu\", kernel_initializer='lecun_uniform', name='enc_1')(x)\n",
    "#x = Dropout(self.dropout)(x)\n",
    "enc = Dense(10, activation=\"linear\", kernel_initializer='lecun_uniform', name='enc_2')(x)\n",
    "\n",
    "x = Dense(25, activation=\"relu\", kernel_initializer='lecun_uniform', name='dec_0')(enc)\n",
    "#x = Dropout(self.dropout)(x)\n",
    "x = Dense(50, activation=\"relu\", kernel_initializer='lecun_uniform', name='dec_1')(x)\n",
    "#x = Dropout(self.dropout)(x)\n",
    "output = Dense(inputShape, activation=\"linear\", kernel_initializer='lecun_uniform', name='dec_2')(x)\n",
    "\n",
    "model = Model(inputs=inputArray, outputs=output)\n",
    "encoder = Model(inputs=inputArray, outputs=enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 102)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 102)               408       \n",
      "_________________________________________________________________\n",
      "enc_0 (Dense)                (None, 50)                5150      \n",
      "_________________________________________________________________\n",
      "enc_1 (Dense)                (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "enc_2 (Dense)                (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dec_0 (Dense)                (None, 25)                275       \n",
      "_________________________________________________________________\n",
      "dec_1 (Dense)                (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dec_2 (Dense)                (None, 102)               5202      \n",
      "=================================================================\n",
      "Total params: 13,870\n",
      "Trainable params: 13,666\n",
      "Non-trainable params: 204\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "encoder.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import h5py\n",
    "import glob\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# 'Generates data for Keras'\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, label, fileList, batch_size, batch_per_file, verbose =0):\n",
    "        self.verbose = verbose\n",
    "        self.label = label\n",
    "        self.fileList = fileList\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_per_file = batch_per_file\n",
    "        # open first file\n",
    "        self.f =  h5py.File(fileList[0],\"r\")\n",
    "        self.X =  np.array(self.f.get('EFP'))\n",
    "        self.X =  np.concatenate((self.X[:,:,0], self.X[:,:,1]))\n",
    "        self.X = shuffle(self.X)\n",
    "        self.y = self.X\n",
    "        self.nBatch = 0\n",
    "        self.iFile = 0\n",
    "        #self.on_epoch_end()\n",
    "\n",
    "    #def on_epoch_end(self):\n",
    "    #    print(\"%s boh\" %self.label)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 'Denotes the number of batches per epoch'\n",
    "        if self.verbose: print(\"%s LEN = %i\" %(self.label, self.batch_per_file*len(self.fileList)))\n",
    "        return self.batch_per_file*len(self.fileList)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        if index == 0:\n",
    "            # reshuffle data\n",
    "            if self.verbose: print(\"%s new epoch\" %self.label)\n",
    "            random.shuffle(self.fileList)\n",
    "            self.iFile = 0\n",
    "            self.nBatch = 0\n",
    "            if self.verbose: print(\"%s new file\" %self.label)\n",
    "            if self.f != None: self.f.close()\n",
    "            self.f = h5py.File(self.fileList[self.iFile], \"r\")\n",
    "            self.X =  np.array(self.f.get('EFP'))\n",
    "            self.X =  np.concatenate((self.X[:,:,0], self.X[:,:,1]))\n",
    "            self.X = shuffle(self.X)\n",
    "            self.y = self.X\n",
    "        if self.verbose: print(\"%s: %i\" %(self.label,index))\n",
    "\n",
    "        #'Generate one batch of data'\n",
    "        iStart = index*self.batch_size\n",
    "        iStop = min(9999, (index+1)*self.batch_size)\n",
    "        if iStop == 9999: iStart = iStop-self.batch_size\n",
    "        myx = self.X[iStart:iStop,:]\n",
    "        myy = self.y[iStart:iStop,:]\n",
    "        if self.nBatch == self.batch_per_file-1:\n",
    "            self.iFile+=1\n",
    "            if self.iFile >= len(self.fileList):\n",
    "                if self.verbose: print(\"%s Already went through all files\" %self.label)\n",
    "            else:\n",
    "                if self.verbose: print(\"%s new file\" %self.label)\n",
    "                self.f.close()\n",
    "                self.f = h5py.File(self.fileList[self.iFile], \"r\")\n",
    "                self.X =  np.array(self.f.get('EFP'))\n",
    "                self.X =  np.concatenate((self.X[:,:,0], self.X[:,:,1]))\n",
    "                self.X = shuffle(self.X)\n",
    "                self.y = self.X\n",
    "            self.nBatch = 0\n",
    "        else:\n",
    "            self.nBatch += 1\n",
    "        return myx, myy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "file_length = minN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_per_file = int(file_length/batch_size)\n",
    "myTrainGen = DataGenerator(\"TRAINING\", X_train, batch_size, my_batch_per_file)\n",
    "myTestGen = DataGenerator(\"TEST\", X_test, batch_size, my_batch_per_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "verbosity = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      " - 2s - loss: 0.0064 - val_loss: 2.2198e-04\n",
      "Epoch 2/5000\n",
      " - 1s - loss: 1.6779e-04 - val_loss: 1.6368e-04\n",
      "Epoch 3/5000\n",
      " - 1s - loss: 1.0791e-04 - val_loss: 6.0769e-05\n",
      "Epoch 4/5000\n",
      " - 1s - loss: 9.1495e-05 - val_loss: 4.3674e-05\n",
      "Epoch 5/5000\n",
      " - 1s - loss: 7.0850e-05 - val_loss: 6.9475e-05\n",
      "Epoch 6/5000\n",
      " - 1s - loss: 8.5471e-05 - val_loss: 4.1998e-05\n",
      "Epoch 7/5000\n",
      " - 1s - loss: 8.9415e-05 - val_loss: 5.9314e-05\n",
      "Epoch 8/5000\n",
      " - 1s - loss: 6.9685e-05 - val_loss: 3.1697e-05\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/5000\n",
      " - 1s - loss: 4.6651e-05 - val_loss: 2.9321e-05\n",
      "Epoch 10/5000\n",
      " - 1s - loss: 4.4920e-05 - val_loss: 2.9716e-05\n",
      "Epoch 11/5000\n",
      " - 1s - loss: 4.6408e-05 - val_loss: 3.9280e-05\n",
      "Epoch 12/5000\n",
      " - 1s - loss: 4.4029e-05 - val_loss: 2.8188e-05\n",
      "Epoch 13/5000\n",
      " - 1s - loss: 4.0652e-05 - val_loss: 2.5476e-05\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 14/5000\n",
      " - 1s - loss: 4.9824e-05 - val_loss: 2.5102e-05\n",
      "Epoch 15/5000\n",
      " - 1s - loss: 4.6185e-05 - val_loss: 2.6241e-05\n",
      "Epoch 16/5000\n",
      " - 1s - loss: 4.2603e-05 - val_loss: 2.4480e-05\n",
      "Epoch 17/5000\n",
      " - 1s - loss: 3.7838e-05 - val_loss: 2.4110e-05\n",
      "Epoch 18/5000\n",
      " - 1s - loss: 3.7841e-05 - val_loss: 2.4169e-05\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 19/5000\n",
      " - 1s - loss: 4.0843e-05 - val_loss: 2.4950e-05\n",
      "Epoch 20/5000\n",
      " - 1s - loss: 4.4576e-05 - val_loss: 2.4530e-05\n",
      "Epoch 21/5000\n",
      " - 1s - loss: 4.4774e-05 - val_loss: 2.5191e-05\n",
      "Epoch 22/5000\n",
      " - 1s - loss: 4.0355e-05 - val_loss: 2.4253e-05\n",
      "Epoch 23/5000\n",
      " - 1s - loss: 4.1102e-05 - val_loss: 2.3902e-05\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 24/5000\n",
      " - 1s - loss: 5.0142e-05 - val_loss: 2.4458e-05\n",
      "Epoch 25/5000\n",
      " - 1s - loss: 4.0836e-05 - val_loss: 2.5001e-05\n",
      "Epoch 26/5000\n",
      " - 1s - loss: 3.9225e-05 - val_loss: 2.3405e-05\n",
      "Epoch 27/5000\n",
      " - 1s - loss: 4.1683e-05 - val_loss: 2.4515e-05\n",
      "Epoch 28/5000\n",
      " - 1s - loss: 4.4456e-05 - val_loss: 2.5416e-05\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 29/5000\n",
      " - 1s - loss: 4.7526e-05 - val_loss: 2.4103e-05\n",
      "Epoch 30/5000\n",
      " - 1s - loss: 5.3464e-05 - val_loss: 2.5237e-05\n",
      "Epoch 31/5000\n",
      " - 1s - loss: 5.2042e-05 - val_loss: 2.3710e-05\n",
      "Epoch 32/5000\n",
      " - 1s - loss: 5.3444e-05 - val_loss: 2.3653e-05\n",
      "Epoch 33/5000\n",
      " - 1s - loss: 4.2722e-05 - val_loss: 2.5199e-05\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 34/5000\n",
      " - 1s - loss: 3.9640e-05 - val_loss: 2.4393e-05\n",
      "Epoch 35/5000\n",
      " - 1s - loss: 4.7145e-05 - val_loss: 2.3638e-05\n",
      "Epoch 36/5000\n",
      " - 1s - loss: 4.3780e-05 - val_loss: 2.4860e-05\n",
      "Epoch 00036: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=myTrainGen, epochs=n_epochs,\n",
    "                    steps_per_epoch= my_batch_per_file*len(X_train), validation_data = myTestGen,\n",
    "                    validation_steps =  my_batch_per_file*len(X_test), verbose=verbosity,\n",
    "                    callbacks = [EarlyStopping(monitor='val_loss', patience=10, verbose=verbosity),\n",
    "                                 ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=verbosity),\n",
    "                                 TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameModel = 'AE_EFP'\n",
    "# store history                                                                                                         \n",
    "f = h5py.File(\"../models/%s_history.h5\" %nameModel, \"w\")\n",
    "f.create_dataset(\"training_loss\", data=np.array(history.history['loss']),compression='gzip')\n",
    "f.create_dataset(\"validation_loss\", data=np.array(history.history['val_loss']),compression='gzip')\n",
    "f.close()\n",
    "\n",
    "# store model                                                                                                           \n",
    "model_json = model.to_json()\n",
    "with open(\"../models/%s.json\" %nameModel, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"../models/%s.h5\" %nameModel)\n",
    "model_json = encoder.to_json()\n",
    "with open(\"../models/%s_ENCODER.json\" %nameModel, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "encoder.save_weights(\"../models/%s_ENCODER.h5\" %nameModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
